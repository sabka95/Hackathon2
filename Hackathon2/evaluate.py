# === IMPORTS ===
import os
from PIL import Image                           # Pour charger les images
import torch                                    # Pour le calcul avec GPU/CPU
import torchvision.transforms as T              # Pour redimensionnement et conversion en tenseur
from torchmetrics.image.fid import FrechetInceptionDistance  # Pour calculer le FID
from torchvision.transforms.functional import pil_to_tensor  # Pour transformer une image PIL en tenseur
import open_clip                                # Pour √©valuer la similarit√© image/texte avec CLIP

# === CONFIGURATION DES DOSSIERS ===
GENERATED_DIR = "images/generated"              # Dossier contenant les images g√©n√©r√©es
REAL_DIR = "images/real"                        # Dossier contenant les images r√©elles de r√©f√©rence
PROMPT_FILE = "test_prompts.txt"                # Fichier contenant les prompts textuels

# === PR√âTRAITEMENT POUR FID ===
transform_fid = T.Compose([
    T.Resize((299, 299)),                       # Redimensionne √† la taille attendue par Inception
    T.ToTensor()                                # Convertit l‚Äôimage en tenseur PyTorch
])

# === INITIALISATION DU CALCUL DU FID ===
fid = FrechetInceptionDistance(feature=64)      # Plus feature est bas, plus c‚Äôest rapide (moins pr√©cis que 2048)

# === INITIALISATION DE CLIP ===
device = "cuda" if torch.cuda.is_available() else "cpu"   # Utilise le GPU si dispo
clip_model, _, preprocess_clip = open_clip.create_model_and_transforms(
    'ViT-B-32', pretrained='laion2b_s34b_b79k')            # Chargement du mod√®le CLIP et de ses transforms
tokenizer = open_clip.get_tokenizer('ViT-B-32')            # Tokeniseur pour transformer le texte en vecteurs
clip_model = clip_model.to(device)                         # Envoie le mod√®le sur le bon p√©riph√©rique

# === AJOUT DES IMAGES R√âELLES POUR FID (base de comparaison) ===
real_images = sorted([
    f for f in os.listdir(REAL_DIR) if f.lower().endswith((".png", ".jpg", ".jpeg"))
])

for fname in real_images:
    img_path = os.path.join(REAL_DIR, fname)
    try:
        img = Image.open(img_path).convert("RGB").resize((299, 299))   # Pr√©traitement image r√©elle
        tensor = pil_to_tensor(img)                                    # Conversion en tenseur (uint8)
        fid.update(tensor.unsqueeze(0), real=True)                     # Ajout √† la r√©f√©rence FID
    except Exception as e:
        print(f"‚ùå Erreur image r√©elle '{fname}' : {e}")

# === CHARGEMENT DES PROMPTS ASSOCI√âS AUX IMAGES G√âN√âR√âES ===
with open(PROMPT_FILE, "r", encoding="utf-8") as f:
    prompts = [line.strip() for line in f.readlines() if line.strip()]  # Nettoyage des lignes vides

# === √âVALUATION DES IMAGES G√âN√âR√âES ===
clip_scores = []                          # Stocke les scores CLIP pour calculer la moyenne
total = len(prompts)                      # Nombre total d‚Äô√©valuations attendues

print(f"\nüîç √âvaluation sur {total} prompts...")

for i, prompt in enumerate(prompts):
    img_path = os.path.join(GENERATED_DIR, f"{i}.png")     # Chemin de l‚Äôimage g√©n√©r√©e correspondante

    if not os.path.exists(img_path):
        print(f"‚ùå Image manquante : {img_path}")
        continue

    try:
        # === FID ===
        image_pil = Image.open(img_path).convert("RGB").resize((299, 299))
        img_fid = pil_to_tensor(image_pil)
        fid.update(img_fid.unsqueeze(0), real=False)       # Ajout √† l‚Äô√©chantillon FID √† comparer

        # === CLIPScore ===
        image = preprocess_clip(image_pil).unsqueeze(0).to(device)   # Pr√©traitement image
        text = tokenizer([prompt]).to(device)                        # Tokenisation du prompt

        with torch.no_grad():                                       # Pas besoin de gradients
            image_feat = clip_model.encode_image(image)             # Embedding image
            text_feat = clip_model.encode_text(text)                # Embedding texte
            score = torch.nn.functional.cosine_similarity(image_feat, text_feat)  # Similarit√©
            clip_scores.append(score.item())                        # Sauvegarde du score

        print(f"[{i+1}/{total}] ‚úÖ {img_path} | CLIPScore: {score.item():.4f}")

    except Exception as e:
        print(f"[{i+1}/{total}] ‚ùå Erreur sur '{img_path}' : {e}")

# === AFFICHAGE DES R√âSULTATS FINAUX ===
mean_clip = sum(clip_scores) / len(clip_scores) if clip_scores else 0
fid_score = fid.compute().item()  # Calcul final du FID

print("\n=== üîö R√âSULTATS FINAUX ===")
print(f"üéØ CLIPScore moyen : {mean_clip:.4f}")   # Score de similarit√© moyen entre image et prompt
print(f"üéØ FID Score       : {fid_score:.4f}")   # Score de distance entre vraies et fausses images
